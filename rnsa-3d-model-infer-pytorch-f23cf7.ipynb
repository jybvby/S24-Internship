{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":36363,"databundleVersionId":4050810,"sourceType":"competition"},{"sourceId":4019072,"sourceType":"datasetVersion","datasetId":2381843},{"sourceId":4136821,"sourceType":"datasetVersion","datasetId":2438889},{"sourceId":4264054,"sourceType":"datasetVersion","datasetId":2406209},{"sourceId":5718655,"sourceType":"datasetVersion","datasetId":2373279}],"dockerImageVersionId":30236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup","metadata":{}},{"cell_type":"markdown","source":"### Links","metadata":{}},{"cell_type":"markdown","source":"Jirka's links:\n* [SpineðŸ¦´Fracture: EDAðŸ”Ž & loading DICOM & 3D browse](https://www.kaggle.com/code/jirkaborovec/spine-fracture-eda-loading-dicom-3d-browse)\n* [SpineðŸ¦´Fracture: convertðŸ¤– DICOM imgs -> 3D volume](https://www.kaggle.com/code/jirkaborovec/spine-fracture-convert-dicom-imgs-3d-volume)\n* [SpineðŸ¦´Fracture: convertðŸ¤– DICOM -> equalized PNG](https://www.kaggle.com/code/jirkaborovec/spine-fracture-convert-dicom-equalized-png)\n* [SpineFracðŸ¦´Classif: e2e ~ Lightningâš¡MONAIâš•ï¸3D](https://www.kaggle.com/code/jirkaborovec/spinefrac-classif-e2e-lightning-monai-3d/notebook)\n* [Cervical Spine Fracture Detection: 3D volumes](https://www.kaggle.com/datasets/jirkaborovec/cervical-spine-fracture-detection-npz-3d-volumes)\n* [Cervical Spine Fracture Detection: equalized PNG](https://www.kaggle.com/datasets/jirkaborovec/cervical-spine-fracture-detection-equalized-png)\n\nSam's links:\n\n* [ðŸ¦´ RSNA Fracture Detection - in-depth EDA](https://www.kaggle.com/code/samuelcortinhas/rsna-fracture-detection-in-depth-eda)\n* [Extracting Vertebrae C1, ..., C7](https://www.kaggle.com/code/samuelcortinhas/extracting-vertebrae-c1-c7)\n* [RSNA - CT gifs](https://www.kaggle.com/code/samuelcortinhas/rsna-ct-gifs)\n* [RSNA 2022 Spine Fracture Detection - Metadata](https://www.kaggle.com/datasets/samuelcortinhas/rsna-2022-spine-fracture-detection-metadata)\n* [RSNA - 3D train tensors [first half]](https://www.kaggle.com/datasets/samuelcortinhas/rsna-3d-train-tensors-first-half)\n* [RSNA - 3D train tensors [second half]](https://www.kaggle.com/datasets/samuelcortinhas/rsna-3d-train-tensors-second-half)\n* [RNSA - 3D model [Train] [PyTorch]](https://www.kaggle.com/code/samuelcortinhas/rnsa-3d-model-train-pytorch)\n* [RSNA - Trained 3D model weights [PyTorch]](https://www.kaggle.com/datasets/samuelcortinhas/rsna-trained-3d-model-weights-pytorch)","metadata":{}},{"cell_type":"markdown","source":"### Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU ../input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl ../input/for-pydicom/pylibjpeg-1.4.0-py3-none-any.whl --find-links frozen_packages --no-index","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-27T13:33:10.175719Z","iopub.execute_input":"2022-08-27T13:33:10.176787Z","iopub.status.idle":"2022-08-27T13:33:18.604854Z","shell.execute_reply.started":"2022-08-27T13:33:10.176603Z","shell.execute_reply":"2022-08-27T13:33:18.603706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle_vol3d_classify -f ../input/cervical-spine-fracture-detection-npz-3d-volumes/frozen_packages --no-index\n# !pip install -qU \"pytorch-lightning>1.5.0\" --no-index\n#!pip uninstall -y torchtext\n#!pip list | grep -e lightning -e kaggle -e monai","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-27T13:33:18.607101Z","iopub.execute_input":"2022-08-27T13:33:18.607376Z","iopub.status.idle":"2022-08-27T13:33:27.113376Z","shell.execute_reply.started":"2022-08-27T13:33:18.607351Z","shell.execute_reply":"2022-08-27T13:33:27.111511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as patches\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.6)\nimport cv2\nimport os\nfrom os import listdir\nimport re\nimport gc\nimport random\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom tqdm.auto import tqdm\nfrom pprint import pprint\nfrom time import time\nimport itertools\nfrom skimage import measure\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nimport nibabel as nib\nfrom glob import glob\nimport warnings\n#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\", category=UserWarning)\n#warnings.filterwarnings(\"ignore\", category=FutureWarning)\nimport zipfile\nfrom scipy import ndimage\nfrom sklearn.model_selection import train_test_split\nfrom joblib import Parallel, delayed\nfrom PIL import Image\nfrom dipy.denoise.nlmeans import nlmeans\nfrom dipy.denoise.noise_estimate import estimate_sigma\nfrom kaggle_volclassif.utils import interpolate_volume\nfrom skimage import exposure\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-27T13:33:27.115134Z","iopub.execute_input":"2022-08-27T13:33:27.115409Z","iopub.status.idle":"2022-08-27T13:33:28.304032Z","shell.execute_reply.started":"2022-08-27T13:33:27.115383Z","shell.execute_reply":"2022-08-27T13:33:28.302996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reproducibility","metadata":{}},{"cell_type":"code","source":"# Set random seeds\ndef set_seed(seed=0):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.305096Z","iopub.execute_input":"2022-08-27T13:33:28.30562Z","iopub.status.idle":"2022-08-27T13:33:28.311643Z","shell.execute_reply.started":"2022-08-27T13:33:28.305592Z","shell.execute_reply":"2022-08-27T13:33:28.310522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nBATCH_SIZE = 1\n\n# Config device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.314864Z","iopub.execute_input":"2022-08-27T13:33:28.31525Z","iopub.status.idle":"2022-08-27T13:33:28.329077Z","shell.execute_reply.started":"2022-08-27T13:33:28.315204Z","shell.execute_reply":"2022-08-27T13:33:28.327662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data","metadata":{}},{"cell_type":"markdown","source":"### Load tables","metadata":{}},{"cell_type":"code","source":"# Load metadata\ntrain_df = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\")\ntrain_bbox = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv\")\ntest_df = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/test.csv\")\nss = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/sample_submission.csv\")\n\n# Print dataframe shapes\nprint('train shape:', train_df.shape)\nprint('train bbox shape:', train_bbox.shape)\nprint('test shape:', test_df.shape)\nprint('ss shape:', ss.shape)\nprint('')\n\n# Show first few entries\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.332217Z","iopub.execute_input":"2022-08-27T13:33:28.332562Z","iopub.status.idle":"2022-08-27T13:33:28.37315Z","shell.execute_reply.started":"2022-08-27T13:33:28.332536Z","shell.execute_reply":"2022-08-27T13:33:28.372241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Debug","metadata":{}},{"cell_type":"code","source":"debug = False\nif len(ss)==3:\n    debug = True\n    \n    # Fix mismatch with test_images folder\n    test_df = pd.DataFrame(columns = ['row_id','StudyInstanceUID','prediction_type'])\n    for i in ['1.2.826.0.1.3680043.22327','1.2.826.0.1.3680043.25399','1.2.826.0.1.3680043.5876']:\n        for j in ['C1','C2','C3','C4','C5','C6','C7','patient_overall']:\n            test_df = test_df.append({'row_id':i+'_'+j,'StudyInstanceUID':i,'prediction_type':j},ignore_index=True)\n    \n    # Sample submission\n    ss = pd.DataFrame(test_df['row_id'])\n    ss['fractured'] = 0.5\n    \n    display(test_df.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.374149Z","iopub.execute_input":"2022-08-27T13:33:28.374537Z","iopub.status.idle":"2022-08-27T13:33:28.431174Z","shell.execute_reply.started":"2022-08-27T13:33:28.374504Z","shell.execute_reply":"2022-08-27T13:33:28.42993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load volumes","metadata":{}},{"cell_type":"code","source":"# Convert dicom images to 3d tensor\ndef convert_volume(dir_path, out_dir = \"test_volumes\", size = (224, 224, 224)):\n    ls_imgs = glob(os.path.join(dir_path, \"*.dcm\"))\n    ls_imgs = sorted(ls_imgs, key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))\n\n    imgs = []\n    for p_img in ls_imgs:\n        dicom = pydicom.dcmread(p_img)\n        img = apply_voi_lut(dicom.pixel_array, dicom)\n        img = cv2.resize(img, size[:2], interpolation=cv2.INTER_LINEAR)\n        imgs.append(img.tolist())\n    vol = torch.tensor(imgs, dtype=torch.float32)\n\n    vol = (vol - vol.min()) / float(vol.max() - vol.min())\n    vol = interpolate_volume(vol, size).numpy()\n    \n    # https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_adapt_hist_eq_3d.html\n    vol = exposure.equalize_adapthist(vol, kernel_size=np.array([64, 64, 64]), clip_limit=0.01)\n    # vol = exposure.equalize_hist(vol)\n    vol = np.clip(vol * 255, 0, 255).astype(np.uint8)\n    \n    path_pt = os.path.join(out_dir, f\"{os.path.basename(dir_path)}.pt\")\n    torch.save(torch.tensor(vol), path_pt)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.432412Z","iopub.execute_input":"2022-08-27T13:33:28.432702Z","iopub.status.idle":"2022-08-27T13:33:28.442517Z","shell.execute_reply.started":"2022-08-27T13:33:28.432669Z","shell.execute_reply":"2022-08-27T13:33:28.441128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make directory\nos.mkdir('/kaggle/working/test_volumes')\n\n# Get paths\nls_dirs = [p for p in glob(os.path.join(\"../input/rsna-2022-cervical-spine-fracture-detection\", \"test_images\", \"*\")) if os.path.isdir(p)]\nprint(f\"volumes: {len(ls_dirs)}\")\n\n# Convert volumes\n_= Parallel(n_jobs=3)(delayed(convert_volume)(p_dir, out_dir='/kaggle/working/test_volumes') for p_dir in tqdm(ls_dirs))","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.443802Z","iopub.execute_input":"2022-08-27T13:33:28.444355Z","iopub.status.idle":"2022-08-27T13:33:28.589366Z","shell.execute_reply.started":"2022-08-27T13:33:28.444315Z","shell.execute_reply":"2022-08-27T13:33:28.58783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Torch dataset","metadata":{}},{"cell_type":"code","source":"# Dataset for test set only\nclass RSNADataset(Dataset):\n    # Initialise\n    def __init__(self, subset='test', df_table=test_df):\n        super().__init__()\n        \n        self.subset = subset\n        self.df_table = df_table\n        \n        # Image paths\n        self.volume_dir = '/kaggle/working/test_volumes/'\n        \n    # Get item in position given by index\n    def __getitem__(self, index):\n        \n        # load 3d volume\n        patient = self.df_table.loc[index,'StudyInstanceUID']\n        path = os.path.join(self.volume_dir, f'{patient}.pt')\n        vol = torch.load(path).to(torch.float32)\n        \n        return (vol.unsqueeze(0), patient)\n\n    # Length of dataset\n    def __len__(self):\n        return len(self.df_table)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.590257Z","iopub.status.idle":"2022-08-27T13:33:28.590585Z","shell.execute_reply.started":"2022-08-27T13:33:28.590416Z","shell.execute_reply":"2022-08-27T13:33:28.59043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test dataset\ntest_table = pd.DataFrame(pd.unique(test_df['StudyInstanceUID']),columns=['StudyInstanceUID'])\ntest_dataset = RSNADataset(subset='test', df_table = test_table)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.59131Z","iopub.status.idle":"2022-08-27T13:33:28.591656Z","shell.execute_reply.started":"2022-08-27T13:33:28.591465Z","shell.execute_reply":"2022-08-27T13:33:28.591482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Torch dataloader","metadata":{}},{"cell_type":"code","source":"# Dataloader\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.592608Z","iopub.status.idle":"2022-08-27T13:33:28.5929Z","shell.execute_reply.started":"2022-08-27T13:33:28.592757Z","shell.execute_reply":"2022-08-27T13:33:28.59277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model\n\nconv output size = floor((W-F+2P)/S + 1)","metadata":{}},{"cell_type":"code","source":"# 3D convolutional neural network\nclass Conv3DNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Layers\n        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=7, stride=1, padding=0)\n        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n        self.norm1 = nn.BatchNorm3d(num_features=16)\n        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n        self.norm2 = nn.BatchNorm3d(num_features=32)\n        self.conv3 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n        self.norm3 = nn.BatchNorm3d(num_features=64)\n        self.avg = nn.AdaptiveAvgPool3d((7, 1, 1))\n        self.flat = nn.Flatten()\n        self.relu = nn.ReLU()\n        self.lin1 = nn.Linear(in_features=448, out_features=128)\n        self.lin2 = nn.Linear(in_features=128, out_features=8)\n        \n    def forward(self, x):\n        # Conv block 1\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.pool(out)\n        out = self.norm1(out)\n        \n        # Conv block 2\n        out = self.conv2(out)\n        out = self.relu(out)\n        out = self.pool(out)\n        out = self.norm2(out)\n        \n        # Conv block 3\n        out = self.conv3(out)\n        out = self.relu(out)\n        out = self.pool(out)\n        out = self.norm3(out)\n        \n        # Average & flatten\n        out = self.avg(out)\n        out = self.flat(out)\n        \n        # Fully connected layer\n        out = self.lin1(out)\n        out = self.relu(out)\n        \n        # Output layer (no sigmoid needed)\n        out = self.lin2(out)\n        \n        return out\n\nmodel = Conv3DNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.593669Z","iopub.status.idle":"2022-08-27T13:33:28.593972Z","shell.execute_reply.started":"2022-08-27T13:33:28.593828Z","shell.execute_reply":"2022-08-27T13:33:28.593842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"# Load checkpoint\nPATH='../input/rsna-trained-3d-model-weights-pytorch/Conv3DNet.pt'\nif torch.cuda.is_available():\n    checkpoint = torch.load(PATH)\nelse:\n    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n\n# Load states\nmodel.load_state_dict(checkpoint['model_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nval_loss = checkpoint['val_loss']\n\n# Evaluation mode\nmodel.eval()\nmodel.to(device)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-27T13:33:28.594651Z","iopub.status.idle":"2022-08-27T13:33:28.595043Z","shell.execute_reply.started":"2022-08-27T13:33:28.594793Z","shell.execute_reply":"2022-08-27T13:33:28.594805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print final loss and epoch\nprint('Final epoch:', epoch)\nprint('Final loss:', loss)\nprint('Final valid loss:', val_loss)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.596285Z","iopub.status.idle":"2022-08-27T13:33:28.596601Z","shell.execute_reply.started":"2022-08-27T13:33:28.596434Z","shell.execute_reply":"2022-08-27T13:33:28.596464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference on test set","metadata":{}},{"cell_type":"code","source":"test_df['fractured']=0.5\nwith torch.no_grad():\n    # Loop over batches\n    for i, (imgs, patient) in enumerate(test_loader):\n        print(f'Iteration {i+1}/{len(test_loader)}')\n        # Send to device\n        imgs = imgs.to(device)\n        \n        # Make predictions\n        preds = model(imgs)\n        \n        # Apply sigmoid\n        sig = nn.Sigmoid()\n        preds = sig(preds)\n        preds = preds.to('cpu')\n        \n        # Save preds\n        test_df.loc[test_df['StudyInstanceUID']==patient[0],'fractured'] = preds.numpy().squeeze()\n        \nprint('Inference complete!')","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.597483Z","iopub.status.idle":"2022-08-27T13:33:28.597775Z","shell.execute_reply.started":"2022-08-27T13:33:28.597623Z","shell.execute_reply":"2022-08-27T13:33:28.597643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"submission = test_df[['row_id','fractured']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-08-27T13:33:28.598422Z","iopub.status.idle":"2022-08-27T13:33:28.598709Z","shell.execute_reply.started":"2022-08-27T13:33:28.598573Z","shell.execute_reply":"2022-08-27T13:33:28.598586Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":36363,"databundleVersionId":4050810,"sourceType":"competition"},{"sourceId":4019072,"sourceType":"datasetVersion","datasetId":2381843},{"sourceId":4136821,"sourceType":"datasetVersion","datasetId":2438889},{"sourceId":4264054,"sourceType":"datasetVersion","datasetId":2406209},{"sourceId":5718655,"sourceType":"datasetVersion","datasetId":2373279}],"dockerImageVersionId":30236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup","metadata":{}},{"cell_type":"markdown","source":"### Links","metadata":{}},{"cell_type":"markdown","source":"Jirka's links:\n* [SpineðŸ¦´Fracture: EDAðŸ”Ž & loading DICOM & 3D browse](https://www.kaggle.com/code/jirkaborovec/spine-fracture-eda-loading-dicom-3d-browse)\n* [SpineðŸ¦´Fracture: convertðŸ¤– DICOM imgs -> 3D volume](https://www.kaggle.com/code/jirkaborovec/spine-fracture-convert-dicom-imgs-3d-volume)\n* [SpineðŸ¦´Fracture: convertðŸ¤– DICOM -> equalized PNG](https://www.kaggle.com/code/jirkaborovec/spine-fracture-convert-dicom-equalized-png)\n* [SpineFracðŸ¦´Classif: e2e ~ Lightningâš¡MONAIâš•ï¸3D](https://www.kaggle.com/code/jirkaborovec/spinefrac-classif-e2e-lightning-monai-3d/notebook)\n* [Cervical Spine Fracture Detection: 3D volumes](https://www.kaggle.com/datasets/jirkaborovec/cervical-spine-fracture-detection-npz-3d-volumes)\n* [Cervical Spine Fracture Detection: equalized PNG](https://www.kaggle.com/datasets/jirkaborovec/cervical-spine-fracture-detection-equalized-png)\n\nSam's links:\n\n* [ðŸ¦´ RSNA Fracture Detection - in-depth EDA](https://www.kaggle.com/code/samuelcortinhas/rsna-fracture-detection-in-depth-eda)\n* [Extracting Vertebrae C1, ..., C7](https://www.kaggle.com/code/samuelcortinhas/extracting-vertebrae-c1-c7)\n* [RSNA - CT gifs](https://www.kaggle.com/code/samuelcortinhas/rsna-ct-gifs)\n* [RSNA 2022 Spine Fracture Detection - Metadata](https://www.kaggle.com/datasets/samuelcortinhas/rsna-2022-spine-fracture-detection-metadata)\n* [RSNA - 3D train tensors [first half]](https://www.kaggle.com/datasets/samuelcortinhas/rsna-3d-train-tensors-first-half)\n* [RSNA - 3D train tensors [second half]](https://www.kaggle.com/datasets/samuelcortinhas/rsna-3d-train-tensors-second-half)\n* [RNSA - 3D model [Train] [PyTorch]](https://www.kaggle.com/code/samuelcortinhas/rnsa-3d-model-train-pytorch)\n* [RSNA - Trained 3D model weights [PyTorch]](https://www.kaggle.com/datasets/samuelcortinhas/rsna-trained-3d-model-weights-pytorch)","metadata":{}},{"cell_type":"markdown","source":"### Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU ../input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl ../input/for-pydicom/pylibjpeg-1.4.0-py3-none-any.whl --find-links frozen_packages --no-index","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-11T23:20:29.891172Z","iopub.execute_input":"2024-07-11T23:20:29.892202Z","iopub.status.idle":"2024-07-11T23:20:32.663571Z","shell.execute_reply.started":"2024-07-11T23:20:29.892094Z","shell.execute_reply":"2024-07-11T23:20:32.662365Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Requirement '../input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle_vol3d_classify -f ../input/cervical-spine-fracture-detection-npz-3d-volumes/frozen_packages --no-index\n!pip install kaggle\n# !pip install -qU \"pytorch-lightning>1.5.0\" --no-index\n#!pip uninstall -y torchtext\n#!pip list | grep -e lightning -e kaggle -e monai","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-11T23:20:32.665913Z","iopub.execute_input":"2024-07-11T23:20:32.666277Z","iopub.status.idle":"2024-07-11T23:21:24.773880Z","shell.execute_reply.started":"2024-07-11T23:20:32.666242Z","shell.execute_reply":"2024-07-11T23:21:24.772669Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nflax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: kaggle in /opt/conda/lib/python3.7/site-packages (1.5.12)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle) (6.1.2)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.26.12)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle) (2022.6.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.28.1)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.64.0)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.15.0)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as patches\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.6)\nimport cv2\nimport os\nfrom os import listdir\nimport re\nimport gc\nimport random\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom tqdm.auto import tqdm\nfrom pprint import pprint\nfrom time import time\nimport itertools\nfrom skimage import measure\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nimport nibabel as nib\nfrom glob import glob\nimport warnings\n#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\", category=UserWarning)\n#warnings.filterwarnings(\"ignore\", category=FutureWarning)\nimport zipfile\nfrom scipy import ndimage\nfrom sklearn.model_selection import train_test_split\nfrom joblib import Parallel, delayed\nfrom PIL import Image\nfrom dipy.denoise.nlmeans import nlmeans\nfrom dipy.denoise.noise_estimate import estimate_sigma\nfrom kaggle_volclassif.utils import interpolate_volume\nfrom skimage import exposure\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-11T23:21:24.775460Z","iopub.execute_input":"2024-07-11T23:21:24.775793Z","iopub.status.idle":"2024-07-11T23:21:26.969917Z","shell.execute_reply.started":"2024-07-11T23:21:24.775748Z","shell.execute_reply":"2024-07-11T23:21:26.968839Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Reproducibility","metadata":{}},{"cell_type":"code","source":"# Set random seeds\ndef set_seed(seed=0):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:21:26.973122Z","iopub.execute_input":"2024-07-11T23:21:26.973762Z","iopub.status.idle":"2024-07-11T23:21:26.981921Z","shell.execute_reply.started":"2024-07-11T23:21:26.973725Z","shell.execute_reply":"2024-07-11T23:21:26.980822Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nBATCH_SIZE = 1\n\n# Config device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:21:26.983481Z","iopub.execute_input":"2024-07-11T23:21:26.984204Z","iopub.status.idle":"2024-07-11T23:21:27.054280Z","shell.execute_reply.started":"2024-07-11T23:21:26.984160Z","shell.execute_reply":"2024-07-11T23:21:27.053083Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2. Data","metadata":{}},{"cell_type":"markdown","source":"### Load tables","metadata":{}},{"cell_type":"code","source":"# Load metadata\ntrain_df = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\")\ntrain_bbox = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv\")\ntest_df = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/test.csv\")\nss = pd.read_csv(\"../input/rsna-2022-cervical-spine-fracture-detection/sample_submission.csv\")\n\n# Print dataframe shapes\nprint('train shape:', train_df.shape)\nprint('train bbox shape:', train_bbox.shape)\nprint('test shape:', test_df.shape)\nprint('ss shape:', ss.shape)\nprint('')\n\n# Show first few entries\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:21:27.055921Z","iopub.execute_input":"2024-07-11T23:21:27.056276Z","iopub.status.idle":"2024-07-11T23:21:27.135748Z","shell.execute_reply.started":"2024-07-11T23:21:27.056244Z","shell.execute_reply":"2024-07-11T23:21:27.134550Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"train shape: (2019, 9)\ntrain bbox shape: (7217, 6)\ntest shape: (3, 3)\nss shape: (3, 2)\n\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>patient_overall</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.6200</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.27262</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.21561</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Debug","metadata":{}},{"cell_type":"code","source":"debug = False\nif len(ss)==3:\n    debug = True\n    \n    # Fix mismatch with test_images folder\n    test_df = pd.DataFrame(columns = ['row_id','StudyInstanceUID','prediction_type'])\n    for i in ['1.2.826.0.1.3680043.22327','1.2.826.0.1.3680043.25399','1.2.826.0.1.3680043.5876']:\n        for j in ['C1','C2','C3','C4','C5','C6','C7','patient_overall']:\n            test_df = test_df.append({'row_id':i+'_'+j,'StudyInstanceUID':i,'prediction_type':j},ignore_index=True)\n    \n    # Sample submission\n    ss = pd.DataFrame(test_df['row_id'])\n    ss['fractured'] = 0.5\n    \n    display(test_df.head(3))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:21:27.137226Z","iopub.execute_input":"2024-07-11T23:21:27.137626Z","iopub.status.idle":"2024-07-11T23:21:27.216787Z","shell.execute_reply.started":"2024-07-11T23:21:27.137584Z","shell.execute_reply":"2024-07-11T23:21:27.215422Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"                         row_id           StudyInstanceUID prediction_type\n0  1.2.826.0.1.3680043.22327_C1  1.2.826.0.1.3680043.22327              C1\n1  1.2.826.0.1.3680043.22327_C2  1.2.826.0.1.3680043.22327              C2\n2  1.2.826.0.1.3680043.22327_C3  1.2.826.0.1.3680043.22327              C3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>StudyInstanceUID</th>\n      <th>prediction_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.22327_C1</td>\n      <td>1.2.826.0.1.3680043.22327</td>\n      <td>C1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.22327_C2</td>\n      <td>1.2.826.0.1.3680043.22327</td>\n      <td>C2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.22327_C3</td>\n      <td>1.2.826.0.1.3680043.22327</td>\n      <td>C3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load volumes","metadata":{}},{"cell_type":"code","source":"# Convert dicom images to 3d tensor\ndef convert_volume(dir_path, out_dir = \"test_volumes\", size = (224, 224, 224)):\n    ls_imgs = glob(os.path.join(dir_path, \"*.dcm\"))\n    ls_imgs = sorted(ls_imgs, key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))\n\n    imgs = []\n    for p_img in ls_imgs:\n        dicom = pydicom.dcmread(p_img)\n        img = apply_voi_lut(dicom.pixel_array, dicom)\n        img = cv2.resize(img, size[:2], interpolation=cv2.INTER_LINEAR)\n        imgs.append(img.tolist())\n    vol = torch.tensor(imgs, dtype=torch.float32)\n\n    vol = (vol - vol.min()) / float(vol.max() - vol.min())\n    vol = interpolate_volume(vol, size).numpy()\n    \n    # https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_adapt_hist_eq_3d.html\n    vol = exposure.equalize_adapthist(vol, kernel_size=np.array([64, 64, 64]), clip_limit=0.01)\n    # vol = exposure.equalize_hist(vol)\n    vol = np.clip(vol * 255, 0, 255).astype(np.uint8)\n    \n    path_pt = os.path.join(out_dir, f\"{os.path.basename(dir_path)}.pt\")\n    torch.save(torch.tensor(vol), path_pt)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:21:27.218573Z","iopub.execute_input":"2024-07-11T23:21:27.218991Z","iopub.status.idle":"2024-07-11T23:21:27.232503Z","shell.execute_reply.started":"2024-07-11T23:21:27.218943Z","shell.execute_reply":"2024-07-11T23:21:27.231228Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Make directory\ntry:\n    os.mkdir('/kaggle/working/test_volumes')\nexcept FileExistsError:\n    pass\n\n# Get paths\nls_dirs = [p for p in glob(os.path.join(\"../input/rsna-2022-cervical-spine-fracture-detection\", \"test_images\", \"*\")) if os.path.isdir(p)]\nprint(f\"volumes: {len(ls_dirs)}\")\n\n# Convert volumes\n_= Parallel(n_jobs=3)(delayed(convert_volume)(p_dir, out_dir='/kaggle/working/test_volumes') for p_dir in tqdm(ls_dirs))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:21:27.234270Z","iopub.execute_input":"2024-07-11T23:21:27.234631Z","iopub.status.idle":"2024-07-11T23:22:01.649247Z","shell.execute_reply.started":"2024-07-11T23:21:27.234597Z","shell.execute_reply":"2024-07-11T23:22:01.647618Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"volumes: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3aa833bbc948a0b475f30e931e4b3d"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Torch dataset","metadata":{}},{"cell_type":"code","source":"# Dataset for test set only\nclass RSNADataset(Dataset):\n    # Initialise\n    def __init__(self, subset='test', df_table=test_df):\n        super().__init__()\n        \n        self.subset = subset\n        self.df_table = df_table\n        \n        # Image paths\n        self.volume_dir = '/kaggle/working/test_volumes/'\n        \n    # Get item in position given by index\n    def __getitem__(self, index):\n        \n        # load 3d volume\n        patient = self.df_table.loc[index,'StudyInstanceUID']\n        path = os.path.join(self.volume_dir, f'{patient}.pt')\n        vol = torch.load(path).to(torch.float32)\n        \n        return (vol.unsqueeze(0), patient)\n\n    # Length of dataset\n    def __len__(self):\n        return len(self.df_table)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:01.654602Z","iopub.execute_input":"2024-07-11T23:22:01.655028Z","iopub.status.idle":"2024-07-11T23:22:01.665861Z","shell.execute_reply.started":"2024-07-11T23:22:01.654988Z","shell.execute_reply":"2024-07-11T23:22:01.664494Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Test dataset\ntest_table = pd.DataFrame(pd.unique(test_df['StudyInstanceUID']),columns=['StudyInstanceUID'])\ntest_dataset = RSNADataset(subset='test', df_table = test_table)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:01.667313Z","iopub.execute_input":"2024-07-11T23:22:01.667649Z","iopub.status.idle":"2024-07-11T23:22:01.686994Z","shell.execute_reply.started":"2024-07-11T23:22:01.667618Z","shell.execute_reply":"2024-07-11T23:22:01.685642Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Torch dataloader","metadata":{}},{"cell_type":"code","source":"# Dataloader\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:01.688483Z","iopub.execute_input":"2024-07-11T23:22:01.689011Z","iopub.status.idle":"2024-07-11T23:22:01.699911Z","shell.execute_reply.started":"2024-07-11T23:22:01.688956Z","shell.execute_reply":"2024-07-11T23:22:01.698904Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model\n\nconv output size = floor((W-F+2P)/S + 1)","metadata":{}},{"cell_type":"code","source":"# 3D convolutional neural network\nclass Conv3DNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Layers\n        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=7, stride=1, padding=0)\n        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n        self.norm1 = nn.BatchNorm3d(num_features=16)\n        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n        self.norm2 = nn.BatchNorm3d(num_features=32)\n        self.conv3 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n        self.norm3 = nn.BatchNorm3d(num_features=64)\n        self.avg = nn.AdaptiveAvgPool3d((7, 1, 1))\n        self.flat = nn.Flatten()\n        self.relu = nn.ReLU()\n        self.lin1 = nn.Linear(in_features=448, out_features=128)\n        self.lin2 = nn.Linear(in_features=128, out_features=8)\n        \n    def forward(self, x):\n        # Conv block 1\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.pool(out)\n        out = self.norm1(out)\n        \n        # Conv block 2\n        out = self.conv2(out)\n        out = self.relu(out)\n        out = self.pool(out)\n        out = self.norm2(out)\n        \n        # Conv block 3\n        out = self.conv3(out)\n        out = self.relu(out)\n        out = self.pool(out)\n        out = self.norm3(out)\n        \n        # Average & flatten\n        out = self.avg(out)\n        out = self.flat(out)\n        \n        # Fully connected layer\n        out = self.lin1(out)\n        out = self.relu(out)\n        \n        # Output layer (no sigmoid needed)\n        out = self.lin2(out)\n        \n        return out\n\nmodel = Conv3DNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:01.701306Z","iopub.execute_input":"2024-07-11T23:22:01.701643Z","iopub.status.idle":"2024-07-11T23:22:04.822055Z","shell.execute_reply.started":"2024-07-11T23:22:01.701613Z","shell.execute_reply":"2024-07-11T23:22:04.820983Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Load model","metadata":{}},{"cell_type":"code","source":"# Load checkpoint\nPATH='../input/rsna-trained-3d-model-weights-pytorch/Conv3DNet.pt'\nif torch.cuda.is_available():\n    checkpoint = torch.load(PATH)\nelse:\n    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n\n# Load states\nmodel.load_state_dict(checkpoint['model_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nval_loss = checkpoint['val_loss']\n\n# Evaluation mode\nmodel.eval()\nmodel.to(device)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-11T23:22:04.823398Z","iopub.execute_input":"2024-07-11T23:22:04.823729Z","iopub.status.idle":"2024-07-11T23:22:04.902272Z","shell.execute_reply.started":"2024-07-11T23:22:04.823699Z","shell.execute_reply":"2024-07-11T23:22:04.901151Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Conv3DNet(\n  (conv1): Conv3d(1, 16, kernel_size=(7, 7, 7), stride=(1, 1, 1))\n  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n  (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n  (norm3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (avg): AdaptiveAvgPool3d(output_size=(7, 1, 1))\n  (flat): Flatten(start_dim=1, end_dim=-1)\n  (relu): ReLU()\n  (lin1): Linear(in_features=448, out_features=128, bias=True)\n  (lin2): Linear(in_features=128, out_features=8, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Print final loss and epoch\nprint('Final epoch:', epoch)\nprint('Final loss:', loss)\nprint('Final valid loss:', val_loss)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:04.903611Z","iopub.execute_input":"2024-07-11T23:22:04.903996Z","iopub.status.idle":"2024-07-11T23:22:04.910276Z","shell.execute_reply.started":"2024-07-11T23:22:04.903963Z","shell.execute_reply":"2024-07-11T23:22:04.909106Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Final epoch: 10\nFinal loss: 0.5331208145284986\nFinal valid loss: 0.5444260619972882\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference on test set","metadata":{}},{"cell_type":"code","source":"test_df['fractured']=0.5\nwith torch.no_grad():\n    # Loop over batches\n    for i, (imgs, patient) in enumerate(test_loader):\n        print(f'Iteration {i+1}/{len(test_loader)}')\n        # Send to device\n        imgs = imgs.to(device)\n        \n        # Make predictions\n        preds = model(imgs)\n        \n        # Apply sigmoid\n        sig = nn.Sigmoid()\n        preds = sig(preds)\n        preds = preds.to('cpu')\n        \n        # Save preds\n        test_df.loc[test_df['StudyInstanceUID']==patient[0],'fractured'] = preds.numpy().squeeze()\n        \nprint('Inference complete!')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:04.911540Z","iopub.execute_input":"2024-07-11T23:22:04.911946Z","iopub.status.idle":"2024-07-11T23:22:11.298817Z","shell.execute_reply.started":"2024-07-11T23:22:04.911914Z","shell.execute_reply":"2024-07-11T23:22:11.297232Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Iteration 1/3\nIteration 2/3\nIteration 3/3\nInference complete!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"submission = test_df[['row_id','fractured']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T23:22:11.300353Z","iopub.execute_input":"2024-07-11T23:22:11.300828Z","iopub.status.idle":"2024-07-11T23:22:11.322061Z","shell.execute_reply.started":"2024-07-11T23:22:11.300753Z","shell.execute_reply":"2024-07-11T23:22:11.321067Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                         row_id  fractured\n0  1.2.826.0.1.3680043.22327_C1   0.155323\n1  1.2.826.0.1.3680043.22327_C2   0.243284\n2  1.2.826.0.1.3680043.22327_C3   0.101768","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>fractured</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.22327_C1</td>\n      <td>0.155323</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.22327_C2</td>\n      <td>0.243284</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.22327_C3</td>\n      <td>0.101768</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}